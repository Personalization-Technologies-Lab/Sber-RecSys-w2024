{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'recspert'\n",
    "repo = 'Recommender-Systems-Intro-Sber-2023'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import diags, csr_matrix\n",
    "from scipy.sparse.linalg import norm as spnorm\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "# navigating to cloned repo directory in Colab\n",
    "%cd {repo} \n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "# restoring original location\n",
    "%cd - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll split data into 3 parts - training, validation and test.\n",
    "- You will firstly use training and validation to tune your models and finding optimal configuration.\n",
    "- Once a set of optimal hyper-parameters is found, you'll need to recompute your models with it on the joint training+validation dataset and report final quality on the test data.\n",
    "\n",
    "For the test data you simply split one last item from each user. The remaining part goes into training+validation. Likewise, you split it one more time the same way as before to get our dataset for tuning.\n",
    "\n",
    "So the scheme is as follows:\n",
    "1. Tune on the training and evaluate on the validation data. Find optimal config.\n",
    "2. Retrain once on the trainin+validation with the optimal config. Report final quality using the test (holdout) data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepraring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = np.random.RandomState(111)\n",
    "sampling_args = dict(target='timestamp', sample_top=True, random_state=rst)\n",
    "# final test data\n",
    "training_validation_, holdout_ = leave_one_out(data, **sampling_args)\n",
    "# validation data\n",
    "training_, validation_ = leave_one_out(training_validation_, **sampling_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 3 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "training, data_index = transform_indices(training_, 'userid', 'movieid')\n",
    "# split validation data\n",
    "validation = reindex(validation_, data_index.values(), filter_invalid=True)\n",
    "validation = validation.sort_values('userid')\n",
    "# split final test data\n",
    "holdout = reindex(holdout_, data_index.values(), filter_invalid=True)\n",
    "holdout = holdout.sort_values('userid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'users': 'userid',\n",
       " 'items': 'movieid',\n",
       " 'feedback': 'rating',\n",
       " 'n_users': 6040,\n",
       " 'n_items': 3703,\n",
       " 'test_users': array([   0,    1,    2, ..., 6037, 6038, 6039], dtype=int64)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    test_users = validation[data_index['users'].name].drop_duplicates().values\n",
    ")\n",
    "data_description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = data[data_description['feedback']]\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype='f8')\n",
    "\n",
    "def build_svd_model(config, data, data_description):\n",
    "    source_matrix = matrix_from_observations(data, data_description)\n",
    "    ... # <- your code here,\n",
    "    # if you need help completing the code, see PureSVD_analysis.ipynb\n",
    "    return item_factors, singular_values\n",
    "\n",
    "def svd_model_scoring(params, data, data_description):\n",
    "    test_matrix = matrix_from_observations(data, data_description)\n",
    "    test_users = data_description['test_users']\n",
    "    item_factors, sigma = params\n",
    "    scores = test_matrix[test_users].dot(item_factors) @ item_factors.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_config = {'rank': 40}\n",
    "userid = data_description['users']\n",
    "seen_data = training.loc[lambda x: x[userid].isin(data_description[\"test_users\"])]\n",
    "\n",
    "svd_params = build_svd_model(svd_config, training, data_description)\n",
    "svd_scores = svd_model_scoring(svd_params, seen_data, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(svd_scores, seen_data, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09221854304635761, 0.03232668453694944, 0.25492843640291657)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_recs = topn_recommendations(svd_scores, topn=10)\n",
    "model_evaluate(svd_recs, validation, data_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled SVD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement data normalization with scaling factor that reduces the effects of item popularity.\n",
    "- Perform comparison of the two models, which consists of:\n",
    "  - tuning both models using validation data with the same range of rank values (use HR as target metric),\n",
    "  - recomputing the models with optimal configuration on the joint train+validation data,\n",
    "  - reporting final quality on the holdout data using all 3 metrics: HR, MRR, Coverage.\n",
    "\n",
    "Note that for each fixed scaling factor in Scaled SVD you don't need to recompute the model. You can still use simple rank truncation for faster tuning.  \n",
    "Also don't forget to update test users in `data_description` before the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3afa3a53b6c5115441aadb460f6d4b1cc743652d4c25bab805986e920f52c789"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sberrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
