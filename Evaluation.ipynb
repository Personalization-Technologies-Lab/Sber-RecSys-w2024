{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean version available at:  \n",
    "- https://github.com/Personalization-Technologies-Lab/Sber-RecSys-w2024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages:\n",
    "```\n",
    "# polara\n",
    "pip install --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from polara import get_movielens_data\n",
    "\n",
    "import seaborn as sns # for better visual aesthetics\n",
    "sns.set_theme(style='white', context='paper')\n",
    "%config InlineBackend.figure_format = \"svg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a recommender systems framework for conducting the necessary experiments, which includes:\n",
    "- training recsys models\n",
    "- generating recommendations\n",
    "- evaluating recommendations quality\n",
    "- performing model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment protocol can be described in terms of 4 functions:\n",
    "\n",
    "```python\n",
    "# building/training a recommender model\n",
    "model_params = build_func(model_config, trainset, trainset_description)\n",
    "\n",
    "# predicting relevance scores for test user-item pairs\n",
    "model_scores = score_func(model_params, testset, testset_description)\n",
    "\n",
    "# generating top-n recommendations using predcted scores\n",
    "model_recoms = recom_func(model_scores, topn)\n",
    "\n",
    "# evaluating quality of recommendations\n",
    "recs_quality = evaluate_func(model_recoms, holdout, holdout_description)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Essentially, this is the main functionality provided by most of the recommender systems frameworks.**  \n",
    "That's why we can say that we're building a simple recsys framework from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Movielens data again. In this example we take a small dataset with 1 million ratings.  \n",
    "The `get_movielens_data` function can automatically download it for us. Alternatively, you can manually [download the ML-1M](https://grouplens.org/datasets/movielens/) dataset and provide a local path to it as the first argument.  \n",
    "The output is a `pandas.DataFrame` object(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, genres_info = get_movielens_data(\n",
    "    get_genres=True, # get genre information in addition to ratings\n",
    "    split_genres=False,\n",
    "    include_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = genres_info.set_index('movieid')\n",
    "movies.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data[\"rating\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .plot.bar(\n",
    "        title='Ratings distribution',\n",
    "        xlabel='rating', ylabel='count',\n",
    "        rot=0\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_movies = [1, 260, 3948, 3949]\n",
    "(\n",
    "    data\n",
    "    .set_index(pd.to_datetime(data['timestamp'], unit='s')) # use TS as data index\n",
    "    .query('movieid in @select_movies') # select movies\n",
    "    .assign(movie_name=lambda x: x['movieid'].map(movies['movienm'])) # get movie names\n",
    "    .groupby('movie_name')\n",
    "    .resample('M') # aggregate number of ratings by month for each movie\n",
    "    .size() # total count\n",
    "    .unstack(level=0)\n",
    "    .plot(logy=True, title='Movie popularity', xlabel='Month', ylabel='# ratings')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see ratings distribution by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genres = movies[\"genres\"].str.split('|') # get a list of genres for each movie\n",
    "# gather and group all movies per each genre\n",
    "genre_movies = movie_genres.explode().to_frame().groupby('genres').groups\n",
    "\n",
    "genre_ratings = {}\n",
    "for genre, movie_list in genre_movies.items():\n",
    "    # caculate average rating per genre\n",
    "    genre_ratings[genre] = (\n",
    "        data\n",
    "        .query('movieid in @movie_list')\n",
    "        .groupby('movieid')[\"rating\"]\n",
    "        .mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average movie rating distribution\n",
    "ratings_hist = data.groupby('movieid')['rating'].mean()\n",
    "ax = ratings_hist.hist(\n",
    "    bins=25, grid=False, edgecolor='b', density=True, label ='Overall'\n",
    ")\n",
    "\n",
    "# plot average movie rating distribution per genre\n",
    "for genre, rating_dist in genre_ratings.items():\n",
    "    rating_dist.plot(grid=True, alpha=0.9, kind='kde', label=genre, ax=ax)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlim(0,5)\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_title('Rating Density plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use a simple temporal \"leave-last-out\" scheme for holdout sampling.  \n",
    "Note that this scheme is not perfect as it's prone to test data leakage from future interactions. We'll deal with it later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split (leaky but simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_last_out(data, userid='userid', timeid='timestamp'):\n",
    "    data_sorted = data.sort_values(timeid)\n",
    "    holdout = data_sorted.drop_duplicates(subset=[userid], keep='last') # split the last item from each user's history\n",
    "    remaining = data.drop(holdout.index) # store the remaining data - will be our training\n",
    "    return remaining, holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_, holdout_ = leave_last_out(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify correct splitting: holdout item must have later timestamps for any user\n",
    "holdout_ts = holdout_.set_index('userid')['timestamp']\n",
    "training_ts = training_.groupby('userid')['timestamp'].max()\n",
    "assert holdout_ts.ge(training_ts).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice,  \n",
    "- source data is not always numeric;\n",
    "- moreover, user and item ids may contain large gaps.\n",
    "\n",
    "We fix that by introducing an internal contiguous numeric index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_indices(data: pd.DataFrame, users: str, items:str, inplace: bool=False):\n",
    "    '''\n",
    "    Reindex columns that correspond to users and items index.\n",
    "    New index is contiguous starting from 0.\n",
    "    '''\n",
    "    data_index = {}\n",
    "    data_codes = {}\n",
    "    for entity, field in zip(['users', 'items'], [users, items]):\n",
    "        new_index, data_index[entity] = to_numeric_id(data, field)\n",
    "        if inplace:\n",
    "            data.loc[:, field] = new_index\n",
    "        else:\n",
    "            data_codes[field] = new_index\n",
    "\n",
    "    if data_codes:\n",
    "        data = data.assign(**data_codes) # makes a copy of data\n",
    "    return data, data_index\n",
    "\n",
    "def to_numeric_id(data: pd.DataFrame, field: str):\n",
    "    '''\n",
    "    Get new contiguous index by converting the data field\n",
    "    into categorical values.\n",
    "    '''\n",
    "    idx_data = data[field].astype(\"category\")\n",
    "    idx = idx_data.cat.codes\n",
    "    idx_map = idx_data.cat.categories.rename(field)\n",
    "    return idx, idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, data_index = transform_indices(training_, 'userid', 'movieid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained new index can now also be applied to the other parts of the dataset to align them with the training data. Let's implement the corresponding reindexing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_data(\n",
    "        data: pd.DataFrame,\n",
    "        data_index: dict,\n",
    "        entities: Optional[Union[str, list[str]]] = None,\n",
    "        inplace: bool = False\n",
    "    ):\n",
    "    '''\n",
    "    Reindex provided data with the specified index mapping.\n",
    "    By default, will take the name of the fields to reindex from `data_index`.\n",
    "    It is also possible to specify which field to reindex by providing `entities`.\n",
    "    '''\n",
    "    if entities is None:\n",
    "        entities = data_index.keys()\n",
    "    if isinstance(entities, str): # handle single entity provided as a string\n",
    "        entities = [entities]\n",
    "\n",
    "    data_codes = {}\n",
    "    for entity in entities:\n",
    "        entity_index = data_index[entity]\n",
    "        field = entity_index.name # extract the field name\n",
    "        new_index = entity_index.get_indexer(data[field])\n",
    "        if inplace:\n",
    "            data.loc[:, field] = new_index # assign new values inplace\n",
    "        else:\n",
    "            data_codes[field] = new_index # store new values\n",
    "    if data_codes:\n",
    "        data = data.assign(**data_codes) # assign new values by making a copy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = reindex_data(holdout_, data_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data consistency check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that holdout doesn't contain entities that are not present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout.query('movieid<0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all invalid entries\n",
    "holdout_clean = holdout.query('movieid >= 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store useful data characteristics for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name, # user field\n",
    "    items = data_index['items'].name, # item field\n",
    "    test_users = holdout_clean[data_index['users'].name].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random recommendations model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_model(config: dict, trainset: pd.DataFrame, trainset_description: dict):\n",
    "    itemid = trainset_description['items']\n",
    "    n_items = ... # the number of unique items in training data\n",
    "    random_state = np.random.default_rng(config.get('seed', None)) # create random number generators with the specified `seed``\n",
    "    return n_items, random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_model_scoring(params: tuple, testset: pd.DataFrame, testset_description: dict):\n",
    "    n_items, random_state = params\n",
    "    n_users = len(testset_description['test_users'])\n",
    "    scores = random_state.random((n_users, n_items))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model_recom_func(scores: np.ndarray, topn: int=10):\n",
    "    recommendations = np.apply_along_axis(topidx, 1, scores, topn)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def topidx(a: np.ndarray, topn: int):\n",
    "    parted = np.argpartition(a, -topn)[-topn:]\n",
    "    return parted[np.argsort(-a[parted])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_params = build_random_model({'seed': 99}, training, data_description)\n",
    "rnd_scores = random_model_scoring(rnd_params, None, data_description)\n",
    "rnd_recoms = simple_model_recom_func(rnd_scores, topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>  \n",
    "\n",
    "* What's the shape of `rnd_scores` array?\n",
    "* What's the shape of `rnd_recoms` array?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_movies(items_idx):\n",
    "    # we convert internal index to external representation\n",
    "    movie_idx = data_index['items'][items_idx]\n",
    "    return movies.loc[movie_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_movies(rnd_recoms[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple popularity-based model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_popularity_model(config: dict, trainset: pd.DataFrame, trainset_description: dict):\n",
    "    itemid = trainset_description['items']\n",
    "    # create Series object with `itemid`` as index and popularity as values\n",
    "    item_popularity = ... # your code here\n",
    "    return item_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_model_scoring(params: tuple, testset: pd.DataFrame, testset_description: dict):\n",
    "    item_popularity = params\n",
    "    # fill in popularity scores for each item with indices from 0 to n_items-1\n",
    "    n_items = item_popularity.index.max() + 1\n",
    "    popularity_scores = np.zeros(n_items,)\n",
    "    popularity_scores[...] = ... # assign popularity score on ordered item id's\n",
    "    # assign the same scores across all test user\n",
    "    n_users = len(testset_description['test_users'])\n",
    "    scores = np.broadcast_to(popularity_scores.reshape(1, -1), (n_users, n_items))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = data_description['test_users']\n",
    "seen_data = training.query('userid in @test_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_params = build_popularity_model({}, training, data_description)\n",
    "pop_scores = popularity_model_scoring(pop_params, None, data_description)\n",
    "\n",
    "pop_scores = pop_scores.copy()\n",
    "pop_recoms = simple_model_recom_func(pop_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>  \n",
    "How to improve recommendations?  \n",
    "\n",
    "Hint: are we recommending obvious items?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating recommendations quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **HitRate** (HR) и **Mean Reciprocal Rank** (MRR).  \n",
    "*Note*: In the case of a single holdout item per user the latter coincides with the Average Reciprocal HitRate (ARHR) and Mean Average Precision (MAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{HR} = \\frac{1}{\\text{\\# test users}} \\sum_{\\text{test users}}{hit}, \\quad\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "hit = \n",
    "\\begin{gather*}\n",
    "\\begin{cases}\n",
    "  1 & \\text{if holdout item in top-$n$ recommendations,}\\\\    \n",
    "  0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MRR} = \\frac{1}{\\text{\\# test users}} \\sum_{\\text{test users}}{\\frac{1}{\\text{hit rank}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_recoms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark correctly predicted items for random recs\n",
    "rnd_hits_mask = rnd_recoms == holdout_clean[data_description['items']].values.reshape(-1, 1)\n",
    "rnd_hits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_hits_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate HR for the random recs model\n",
    "rnd_hr = rnd_hits_mask.any(axis=1).mean()\n",
    "print(f'{rnd_hr=:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify positions of the correctly guessed items within recommendations\n",
    "_, rnd_hits_rank = np.where(rnd_hits_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_hits_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MRR for the random recs model\n",
    "n_test_users = len(data_description['test_users'])\n",
    "rnd_mrr = (1. / (rnd_hits_rank+1)).sum() / n_test_users\n",
    "print(f'{rnd_mrr=:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(\n",
    "        recommended_items: np.ndarray,\n",
    "        holdout: pd.DataFrame,\n",
    "        holdout_description: dict,\n",
    "        topn: int=10\n",
    "    ):\n",
    "    itemid = holdout_description['items']\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    # HR calculation\n",
    "    hr = ... # compute HR score\n",
    "    # MRR calculation\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = ... # compute MRR score\n",
    "    return hr, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(rnd_recoms, holdout_clean, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(pop_recoms, holdout_clean, data_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>  \n",
    "\n",
    "* The obtained results are metrics@<...>?\n",
    "* What is the obvious way of improving the obtained results? Hint: we miss something in our scoring functions.\n",
    "* How do we get confidence intervals and estimate statistical significance of the results?\n",
    "* What changes do we need to make in order to assess strong generalization?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages:\n",
    "```\n",
    "# ipypb\n",
    "pip install ipypb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipypb import track"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume there's no dependence of user consumption pattern on time (or at least we are not interested in it). In that case, temporal splitting is not necessary and we can switch to e.g. random sampling strategy. \n",
    "\n",
    "Our methodology will be:\n",
    "- random \"leave-one-out\" sampling for holdout items\n",
    "- use $k$-fold cross-validation scheme,\n",
    "- test against strong generalization of models (warm-start scenario)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(data, data_descirption, seed=None):\n",
    "    shuffled = data.sample(frac=1, random_state=seed)\n",
    "    userid = data_descirption['users']\n",
    "    random_holdout = shuffled.drop_duplicates(subset=[userid], keep='last')\n",
    "    remaining = data.drop(random_holdout.index)\n",
    "    return remaining, random_holdout\n",
    "\n",
    "def train_test_split(test_index, data, data_description, seed, holdout_sampling=None):\n",
    "    userid = data_description['users']\n",
    "    itemid = data_description['items']\n",
    "    if holdout_sampling is None:\n",
    "        holdout_sampling = leave_one_out # random holdout\n",
    "    # split train/test and build internal index\n",
    "    test_data_ = data.iloc[test_index]\n",
    "    training_ = data.drop(test_data_.index)\n",
    "    training, data_index = transform_indices(training_, userid, itemid)\n",
    "    # conform test data with internal index, skip items not present in training\n",
    "    # note, we do not need to reindex users as they are not part of the training!\n",
    "    test_data = reindex_data(test_data_, data_index, 'items').query(f'{itemid} >= 0')\n",
    "    # split holdout\n",
    "    testset_, holdout_ = holdout_sampling(test_data, data_description, seed=seed)\n",
    "    # ensure user index consistency in test data\n",
    "    # due to warm-start condition, we don't need to check\n",
    "    # for consistency with the training data\n",
    "    same_test_users = testset_[userid].isin(holdout_[userid])\n",
    "    if not same_test_users.all():\n",
    "        testset_ = testset_[same_test_users]\n",
    "    # sort by users for convenience and conformity\n",
    "    testset = testset_.sort_values(userid)\n",
    "    holdout = holdout_.sort_values(userid)\n",
    "    return training, testset, holdout, data_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['HR', 'MRR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cv(\n",
    "    data, data_description,\n",
    "    models, configs,\n",
    "    build_funcs, score_funcs,\n",
    "    recom_func, eval_func,\n",
    "    *,\n",
    "    k_folds=5,\n",
    "    topn_list=None,\n",
    "    seed=None,\n",
    "    fold_experiment=None,\n",
    "    **kwargs\n",
    "):\n",
    "    # perform basic checks\n",
    "    assert len(configs) == len(models) == len(build_funcs) == len(score_funcs)\n",
    "    if fold_experiment is None:\n",
    "        fold_experiment = run_fold_experiment\n",
    "    if topn_list is None:\n",
    "        topn_list = [10]\n",
    "    # initiate experiment\n",
    "    kfold = GroupKFold(n_splits=k_folds)\n",
    "    user_groups = data[data_description['users']]\n",
    "    results = []\n",
    "    for train_idx, test_idx in track(kfold.split(data, groups=user_groups), k_folds):\n",
    "        fold_data = train_test_split(test_idx, data, data_description, seed)\n",
    "        fold_res = fold_experiment(\n",
    "            fold_data, data_description,\n",
    "            models, configs,\n",
    "            build_funcs, score_funcs,\n",
    "            recom_func, eval_func,\n",
    "            topn_list,\n",
    "            **kwargs\n",
    "        )\n",
    "        results.append(fold_res)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_fold_experiment(\n",
    "    fold_data, data_description,\n",
    "    models,  configs,\n",
    "    build_funcs, score_funcs,\n",
    "    recom_func, eval_func,\n",
    "    topn_list\n",
    "):\n",
    "    fold_results = {model: {metric: [] for metric in metrics} for model in models}\n",
    "    topn_max = max(topn_list)\n",
    "    # unpack experiment data\n",
    "    training, testset, holdout, data_index = fold_data\n",
    "    testset_description = {\n",
    "        **data_description,\n",
    "        'n_test_users': testset[data_description['users']].nunique()\n",
    "    }\n",
    "    # run experiment\n",
    "    for model, build_func, score_func, model_config in zip(models, build_funcs, score_funcs, configs):\n",
    "        model_params = ...\n",
    "        model_scores = ...\n",
    "        model_recoms = ...\n",
    "        for topn in topn_list:\n",
    "            hr, mrr = eval_func(model_recoms, holdout, data_description, topn=topn)\n",
    "            fold_results[model]['HR'].append(hr)\n",
    "            fold_results[model]['MRR'].append(mrr)\n",
    "    return fold_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick reminder on functions signatures:\n",
    "```python\n",
    "pop_params = build_popularity_model({}, training, data_description)\n",
    "pop_scores = popularity_model_scoring(pop_params, None, data_description)\n",
    "pop_recoms = simple_model_recom_func(pop_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['most popular', 'random']\n",
    "model_configs = [{}, {'seed': 0}]\n",
    "model_build_funcs = [build_popularity_model, build_random_model]\n",
    "models_scoring = [popularity_model_scoring, random_model_scoring]\n",
    "topn_list = [1, 5, 10]\n",
    "\n",
    "results = run_kfold_cv(\n",
    "    data, data_description,\n",
    "    model_names, model_configs,\n",
    "    model_build_funcs, models_scoring,\n",
    "    simple_model_recom_func, model_evaluate,\n",
    "    k_folds=4, topn_list=topn_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(results, topn_list):\n",
    "    metric_index = pd.MultiIndex.from_product(\n",
    "        [metrics, topn_list],\n",
    "        names=['metric', 'topn']\n",
    "    )\n",
    "    results_df = pd.concat(\n",
    "        {\n",
    "            foldid: pd.DataFrame(res)\n",
    "            .explode(column=list(res.keys()))\n",
    "            .set_index(metric_index)\n",
    "            for foldid, res in enumerate(results, start=1)\n",
    "        },\n",
    "        names=['fold']\n",
    "    )\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_to_df(results, topn_list)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(level=['metric', 'topn']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(level=['metric', 'topn']).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "\n",
    "- Do we actually measure user satisfaction with the implemented sampling? Why?\n",
    "- Suggest an evaluation protocol to measure potential satisfaction better.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polara.evaluation.evaluation_engine import sample_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ci(results_df.unstack('metric'), level='topn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polara import RecommenderData, RandomModel, PopularityModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most-recent-item split example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = dict(\n",
    "    warm_start = False,\n",
    "    holdout_size = 1,\n",
    "    test_ratio = 0, # split holdout from all users\n",
    "    random_holdout = False, # use either feedback or custom_order field\n",
    ")\n",
    "dm = RecommenderData(\n",
    "    data, 'userid', 'movieid', 'rating',\n",
    "    custom_order='timestamp', config=data_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = PopularityModel(dm)\n",
    "pop.build()\n",
    "pop.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = RandomModel(dm, seed=0)\n",
    "rnd.build()\n",
    "rnd.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polara.evaluation import evaluation_engine as ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = RecommenderData(data, 'userid', 'movieid', 'rating')\n",
    "\n",
    "dm.warm_start = True\n",
    "dm.holdout_size = 1\n",
    "dm.test_ratio = 0.2 # split holdout from all users\n",
    "dm.random_holdout = True # sample randomly\n",
    "dm.verbose = False # don't print data update info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_values = [1, 5, 10]\n",
    "models = [\n",
    "    RandomModel(dm, seed=0),\n",
    "    PopularityModel(dm)\n",
    "]\n",
    "\n",
    "# run CV experiment\n",
    "results = ee.run_cv_experiment(\n",
    "    models,\n",
    "    fold_experiment=ee.topk_test,\n",
    "    topk_list=topk_values,\n",
    "    iterator=track\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[:, :'experience'].groupby(level=['top-n', 'model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downvote_seen_items(scores: np.ndarray, data: pd.DataFrame, data_description: dict):\n",
    "    itemid = data_description['items']\n",
    "    userid = data_description['users']\n",
    "    # get indices of observed data\n",
    "    data_sorted = data.sort_values(userid)\n",
    "    item_idx = data_sorted[itemid].values\n",
    "    user_idx = data_sorted[userid].values\n",
    "    user_idx = np.r_[False, user_idx[1:] != user_idx[:-1]].cumsum()\n",
    "    # downvote scores at the corresponding positions\n",
    "    seen_idx_flat = np.ravel_multi_index((user_idx, item_idx), scores.shape)\n",
    "    np.put(scores, seen_idx_flat, scores.min() - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
