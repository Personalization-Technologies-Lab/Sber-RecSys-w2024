{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'Personalization-Technologies-Lab'\n",
    "repo = 'Sber-RecSys-w2024'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_stackexchange\n",
    "\n",
    "from polara.evaluation.pipelines import random_grid\n",
    "from polara.lib.earlystopping import early_stopping_callback\n",
    "from polara.tools.display import print_frames\n",
    "\n",
    "# navigating to cloned repo directory in Colab\n",
    "%cd {repo}\n",
    "from evaluation import topn_recommendations\n",
    "# restoring original location\n",
    "%cd -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is adapted from official `LightFM`'s documentation for a cold-start scenario:  \n",
    "https://making.lyst.com/lightfm/docs/examples/hybrid_crossvalidated.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the StackExchange data dump. The dataset consists of users and questions they answered.\n",
    "\n",
    "**Task**:  \n",
    "Find users that are most qualified for answering new questions.\n",
    "\n",
    "Your recommendation algorithm must tailor matching between users and questions based on user expertise. You will need to use hybrid approach that utilizes side information about items. The dataset contains question labels in the form of user-assigned `tags`. Hence, even though questions will be \"cold\" (i.e., unanswered), you can still find the best match between experts and questions based on their answering history and tags used in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_stackexchange('crossvalidated',\n",
    "                           test_set_fraction=0.1,\n",
    "                           indicator_features=False,\n",
    "                           tag_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'item_features', 'item_feature_labels'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ['users', 'questions']\n",
    "training_data = pd.DataFrame(dict(zip(entities, data['train'].nonzero())))\n",
    "test_data = pd.DataFrame(dict(zip(entities, data['test'].nonzero())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tags = (\n",
    "    pd.DataFrame(dict(zip(['questions', 'tags'], data['item_features'].nonzero())))\n",
    "    .assign(tags = lambda x: data['item_feature_labels'][x['tags'].values])\n",
    "    .groupby('questions')\n",
    "    ['tags'].apply(list)\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: none\">\n",
       "    <tr style=\"border: none\"><td style=\"border: none\"> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div> </td>\n",
       "<td style=\"border: none\"> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div> </td>\n",
       "<td style=\"border: none\"> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bayesian, prior, elicitation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[distributions, normality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[software, open-source]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[distributions, statistical-significance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[machine-learning]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div> </td>\n",
       "    </tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_frames([training_data.head(), # data for training and validation\n",
    "              test_data.head(), # data for testing\n",
    "              item_tags.head()]) # item features data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset already provides splitting. But an additional step is still required: splitting into validation and actual test parts.\n",
    "\n",
    "To simplify evaluation, only a single true expert will be withheld from each \"cold\" question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(100)\n",
    "final_test = (\n",
    "    test_data\n",
    "    .sample(frac=1, random_state=random_state)\n",
    "    .drop_duplicates(subset=['questions'])\n",
    "    .sample(frac=0.75, random_state=random_state) # make test and validation sizes more balanced\n",
    "    .sort_values('questions')\n",
    ")\n",
    "validation = (\n",
    "    test_data\n",
    "    .drop(final_test.index)\n",
    "    .sample(frac=1, random_state=random_state)\n",
    "    .drop_duplicates(subset=['questions'])\n",
    "    .sort_values('questions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: none\">\n",
       "    <tr style=\"border: none\"><td style=\"border: none\"> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>137</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>2364</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>46</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>1931</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>421</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div> </td>\n",
       "<td style=\"border: none\"> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>84</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>2067</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div> </td>\n",
       "    </tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_frames([validation.head(), final_test.head()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users         345\n",
       "questions    1356\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users         509\n",
       "questions    2823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lfm_model(config, data, data_description, early_stop_config=None, iterator=None):\n",
    "    # the model\n",
    "    model = LightFM(\n",
    "        no_components = config['num_components'],\n",
    "        max_sampled = config['max_sampled'],\n",
    "        loss = config['loss'],\n",
    "        learning_schedule = config['learning_schedule'],\n",
    "        user_alpha = config['user_alpha'],\n",
    "        item_alpha = config['item_alpha']\n",
    "    )\n",
    "    if iterator is None:\n",
    "        iterator = lambda x: x\n",
    "    # early stoppping configuration\n",
    "    es_config = check_early_stop_config(early_stop_config)\n",
    "    # training\n",
    "    for epoch in iterator(range(config['max_epochs'])):\n",
    "        try:\n",
    "            train_lfm_epoch(epoch, model, data, data_description, es_config)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "def check_early_stop_config(early_stop_config):\n",
    "    if early_stop_config is None:\n",
    "        early_stop_config = {}\n",
    "    try:\n",
    "        es_dict = dict(\n",
    "            early_stopper = early_stop_config['evaluation_callback'],\n",
    "            callback_interval = early_stop_config['callback_interval'],\n",
    "            holdout = early_stop_config['holdout'],\n",
    "            stop_early = True\n",
    "        )\n",
    "    except KeyError:\n",
    "        es_dict = dict(stop_early = False)\n",
    "    return es_dict\n",
    "\n",
    "\n",
    "def train_lfm_epoch(\n",
    "    epoch, model, train, data_description, es_config,\n",
    "):\n",
    "    model.fit_partial(\n",
    "        train,\n",
    "        user_features=data_description['user_features'],\n",
    "        item_features=data_description['item_features'],\n",
    "        epochs=1\n",
    "    )\n",
    "    if es_config['stop_early'] and ((epoch+1) % es_config['callback_interval'] == 0):\n",
    "        # evaluate model and raise StopIteration if early stopping condition is met\n",
    "        es_config['early_stopper'](epoch, model, es_config['holdout'], data_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightfm_scoring(model, data, data_description):\n",
    "    dtype = 'i4'\n",
    "    all_users = np.arange(data_description['n_users'], dtype=dtype)\n",
    "    test_items = data_description['cold_items'].astype(dtype)\n",
    "    item_index, user_index = np.meshgrid(test_items, all_users, copy=False)\n",
    "\n",
    "    scores = model.predict(\n",
    "        user_index.ravel(),\n",
    "        item_index.ravel(),\n",
    "        item_features=data_description['item_features'],\n",
    "    ).reshape(len(test_items), len(all_users), order='F')\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is based on the `polara`'s `early_stopping_callback` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coldstart_evaluate(recommended_users, holdout, data_description, topn=10):\n",
    "    userid = data_description['users']\n",
    "    holdout_users = holdout[userid].values\n",
    "    assert recommended_users.shape[0] == len(holdout_users)\n",
    "    hits_mask = recommended_users[:, :topn] == holdout_users.reshape(-1, 1)\n",
    "    # HR calculation\n",
    "    hr = np.mean(hits_mask.any(axis=1))\n",
    "    # MRR calculation\n",
    "    n_test_items = recommended_users.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_items\n",
    "    return {'hr': hr, 'mrr': mrr}\n",
    "\n",
    "def lfm_evaluator(model, holdout, data_description, target_metric='hr'):\n",
    "    lfm_scores = lightfm_scoring(model, None, data_description)\n",
    "    lfm_recs = topn_recommendations(lfm_scores)\n",
    "    metrics = coldstart_evaluate(lfm_recs, holdout, {'users': 'users'})\n",
    "    return metrics[target_metric]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_config = dict(\n",
    "    num_components = 30,\n",
    "    loss = 'warp',\n",
    "    max_sampled = 3,\n",
    "    max_epochs = 100,\n",
    "    learning_schedule = 'adagrad',\n",
    "    user_alpha = 1e-3,\n",
    "    item_alpha = 1e-3,\n",
    ")\n",
    "\n",
    "try_early_stop = early_stopping_callback(\n",
    "        lfm_evaluator, max_fails=3, verbose=True\n",
    ")\n",
    "\n",
    "early_stop_config = dict(\n",
    "    evaluation_callback = try_early_stop,\n",
    "    callback_interval = 10, # break between consequent evaluation in epochs\n",
    "    holdout = validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'users': 'users',\n",
       " 'items': 'questions',\n",
       " 'n_users': 3213,\n",
       " 'cold_items': array([    2,    42,    80, ..., 72314, 72323, 72358], dtype=int32),\n",
       " 'user_features': None,\n",
       " 'item_features': <72360x1246 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 198963 stored elements in Compressed Sparse Row format>}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_description = dict(\n",
    "    users = 'users',\n",
    "    items = 'questions',\n",
    "    n_users = data['train'].shape[0],\n",
    "    cold_items = validation['questions'].values,\n",
    "    user_features = data.get('user_features'),\n",
    "    item_features = data.get('item_features'),\n",
    ")\n",
    "data_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:06<01:07,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 metric score: 0.021386430678466076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:12<01:05,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19 metric score: 0.019174041297935103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:18<00:54,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29 metric score: 0.021386430678466076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:24<00:43,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39 metric score: 0.022123893805309734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:30<00:33,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49 metric score: 0.022861356932153392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:36<00:28,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59 metric score: 0.022123893805309734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:42<00:21,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 69 metric score: 0.021386430678466076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:47<00:12,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 79 metric score: 0.02064896755162242\n",
      "Metric no longer improves. Best score 0.022861356932153392, attained in 50 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lfm_params = build_lfm_model(\n",
    "    lfm_config,\n",
    "    data['train'],\n",
    "    data_description,\n",
    "    early_stop_config=early_stop_config,\n",
    "    iterator=tqdm\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_params_grid = dict(\n",
    "    num_components = [8, 12, 16, 24, 32, 48, 64],\n",
    "    loss = ['warp'],\n",
    "    max_sampled = [3, 10, 30, 100],\n",
    "    max_epochs = [100],\n",
    "    learning_schedule = ['adagrad'],\n",
    "    user_alpha = [1e-5],\n",
    "    item_alpha = [1e-5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid, param_names = random_grid(lfm_params_grid, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:32<00:00, 30.48s/it]\n"
     ]
    }
   ],
   "source": [
    "early_stop_config = dict(\n",
    "    callback_interval = 10, # break between consequent evaluation in epochs\n",
    "    holdout = validation,\n",
    ")\n",
    "\n",
    "lfm_results = {}\n",
    "\n",
    "for grid_params in tqdm(param_grid):\n",
    "    lfm_config = dict(zip(param_names, grid_params))\n",
    "    early_stop_config['evaluation_callback'] = es_call = early_stopping_callback(\n",
    "        lfm_evaluator, max_fails=3, verbose=False\n",
    "    )\n",
    "    lfm_params = build_lfm_model(\n",
    "        lfm_config,\n",
    "        data['train'],\n",
    "        data_description,\n",
    "        early_stop_config=early_stop_config,\n",
    "    )\n",
    "    num_epochs = es_call.iter + 1 # store optimal number of epochs\n",
    "    lfm_results[grid_params+(num_epochs,)] = es_call.target # store optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 'warp', 30, 100, 'adagrad', 1e-05, 1e-05, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_lfm_config = pd.Series(lfm_results).idxmax()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_components': 48,\n",
       " 'loss': 'warp',\n",
       " 'max_sampled': 30,\n",
       " 'max_epochs': 10,\n",
       " 'learning_schedule': 'adagrad',\n",
       " 'user_alpha': 1e-05,\n",
       " 'item_alpha': 1e-05}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfm_optimal_config = dict(zip(param_names, optimal_lfm_config[:-1]))\n",
    "lfm_optimal_config['max_epochs'] = optimal_lfm_config[-1]\n",
    "lfm_optimal_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description, dtype='f4'):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = np.ones(data.shape[0])\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = training_data.append(validation, ignore_index=True)\n",
    "train_matrix = matrix_from_observations(\n",
    "    final_train, data_description, dtype=data['train'].dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "lfm_params = build_lfm_model(\n",
    "    lfm_optimal_config,\n",
    "    train_matrix,\n",
    "    data_description,\n",
    "    early_stop_config=None,\n",
    "    iterator=tqdm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description['cold_items'] = final_test[data_description['items']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_scores = lightfm_scoring(lfm_params, None, data_description)\n",
    "lfm_recs = topn_recommendations(lfm_scores)\n",
    "metrics = coldstart_evaluate(lfm_recs, final_test, data_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr': 0.024087849805171802, 'mrr': 0.008928922850283105}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a217029602e685eff9d4c4695788fb2d6fa824043a2f7919e2a5a2be7c55d9f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('polara_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
